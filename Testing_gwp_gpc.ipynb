{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1941f0"
      },
      "source": [
        "# Task\n",
        "Create a Python script that loads a set of pre-trained machine learning models and a data scaler from pickle files. The script should then prompt the user for input features, including `days_testing`, `fly_ash`, `ggbs`, `sodium_silicate`, `sodium_hydroxide`, `sand`, `coarse_aggregate`, and `glass_waste`. The script should then use these inputs to create a pandas DataFrame, scale the data using the loaded scaler, and then make predictions using each of the loaded models. Finally, the script should display the predictions from all the models. The models to be loaded are:\n",
        "\n",
        "- \"ridge_model.pkl\"\n",
        "- \"decision_tree_model.pkl\"\n",
        "- \"decision_tree_grid_search_model.pkl\"\n",
        "- \"random_forest_model.pkl\"\n",
        "- \"random_forest_grid_search_model.pkl\"\n",
        "- \"ada_boost_model.pkl\"\n",
        "- \"ada_boost_grid_search_model.pkl\"\n",
        "- \"gbr_model.pkl\"\n",
        "- \"gbr_grid_search_model.pkl\"\n",
        "- \"svr_model.pkl\"\n",
        "- \"svr_grid_search_model.pkl\"\n",
        "- \"gpr_model.pkl\"\n",
        "- \"gpr_grid_search_model.pkl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a873b8d2"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "### Subtask:\n",
        "Import the necessary libraries, such as pandas and pickle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deb795f3"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e442ab09"
      },
      "source": [
        "## Load models\n",
        "\n",
        "### Subtask:\n",
        "Load the pre-trained models and the scaler from the pickle files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cc1f4f1",
        "outputId": "e2e4cefb-d9b1-4de2-be01-c761b89806a3"
      },
      "source": [
        "model_filenames = [\n",
        "    \"ada_boost_grid_search_model.pkl\",\n",
        "    \"ada_boost_model.pkl\",\n",
        "    \"decision_tree_model.pkl\",\n",
        "    \"decision_tree_grid_search_model.pkl\",\n",
        "    \"gbr_grid_search_model.pkl\",\n",
        "    \"gbr_model.pkl\",\n",
        "    \"gpr_model.pkl\",\n",
        "    \"gpr_grid_search_model.pkl\",\n",
        "    \"knn_model.pkl\",\n",
        "    \"knn_grid_search_model.pkl\",\n",
        "    \"mlp_model.pkl\",\n",
        "    \"mlp_grid_search_model.pkl\",\n",
        "    \"random_forest_model.pkl\",\n",
        "    \"random_forest_grid_search_model.pkl\",\n",
        "    \"ridge_model.pkl\",\n",
        "    \"scaler.pkl\",\n",
        "    \"svr_model.pkl\",\n",
        "    \"svr_grid_search_model.pkl\",\n",
        "\n",
        "]\n",
        "\n",
        "loaded_models = {}\n",
        "\n",
        "for filename in model_filenames:\n",
        "    with open(filename, 'rb') as f:\n",
        "        loaded_models[filename] = pickle.load(f)\n",
        "\n",
        "with open(\"scaler.pkl\", 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "print(\"Models loaded successfully:\")\n",
        "for model_name in loaded_models:\n",
        "    print(f\"- {model_name}\")\n",
        "print(\"\\nScaler loaded successfully.\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded successfully:\n",
            "- ada_boost_grid_search_model.pkl\n",
            "- ada_boost_model.pkl\n",
            "- decision_tree_model.pkl\n",
            "- decision_tree_grid_search_model.pkl\n",
            "- gbr_grid_search_model.pkl\n",
            "- gbr_model.pkl\n",
            "- gpr_model.pkl\n",
            "- gpr_grid_search_model.pkl\n",
            "- knn_model.pkl\n",
            "- knn_grid_search_model.pkl\n",
            "- mlp_model.pkl\n",
            "- mlp_grid_search_model.pkl\n",
            "- random_forest_model.pkl\n",
            "- random_forest_grid_search_model.pkl\n",
            "- ridge_model.pkl\n",
            "- scaler.pkl\n",
            "- svr_model.pkl\n",
            "- svr_grid_search_model.pkl\n",
            "\n",
            "Scaler loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09da9cab"
      },
      "source": [
        "# Task\n",
        "Create a Python script that builds a testing file for predicting a value based on user-provided input features.\n",
        "\n",
        "The script should:\n",
        "\n",
        "1.  **Define Input Features:**\n",
        "    *   `days_testing` (with options: 7, 28, 90)\n",
        "    *   `fly_ash`\n",
        "    *   `ggbs`\n",
        "    *   `sodium_silicate`\n",
        "    *   `sodium_hydroxide`\n",
        "    *   `sand`\n",
        "    *   `coarse_aggregate`\n",
        "    *   `glass_waste`\n",
        "    *   `curing_type`\n",
        "\n",
        "2.  **Prepare Input Data:**\n",
        "    *   Create a function `prepare_input_data()` that takes the user inputs and converts them into a pandas DataFrame with the following column names:\n",
        "        *   `'Number of days (testing)'`\n",
        "        *   `'fly_ash_percentage'`\n",
        "        *   `'ggbs_percentage'`\n",
        "        *   `'sodium_silicate_percentage'`\n",
        "        *   `'sodium_hydroxide_percentage'`\n",
        "        *   `'sand_percentage'`\n",
        "        *   `'coarse_aggregate_percentage'`\n",
        "        *   `'glass_waste_percentage'`\n",
        "        *   `'Curing_type'`\n",
        "\n",
        "3.  **Scale the Data:**\n",
        "    *   Load a pre-trained scaler from a file.\n",
        "    *   Use the loaded scaler to transform the input DataFrame.\n",
        "\n",
        "4.  **Make Predictions:**\n",
        "    *   Load the following pre-trained models from their respective `.pkl` files:\n",
        "        *   `linear_regression_model.pkl`\n",
        "        *   `ridge_model.pkl`\n",
        "        *   `decision_tree_model.pkl`\n",
        "        *   `decision_tree_grid_search_model.pkl`\n",
        "        *   `random_forest_model.pkl`\n",
        "        *   `random_forest_grid_search_model.pkl`\n",
        "        *   `ada_boost_model.pkl`\n",
        "        *   `ada_boost_grid_search_model.pkl`\n",
        "        *   `gbr_model.pkl`\n",
        "        *   `gbr_grid_search_model.pkl`\n",
        "        *   `svr_model.pkl`\n",
        "        *   `svr_grid_search_model.pkl`\n",
        "        *   `gpr_model.pkl`\n",
        "        *   `gpr_grid_search_model.pkl`\n",
        "    *   Use each loaded model to predict the output based on the scaled input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30fb7b47"
      },
      "source": [
        "## Get user input\n",
        "\n",
        "### Subtask:\n",
        "Create a function to get the input features from the user.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c911285"
      },
      "source": [
        "def get_user_input():\n",
        "    \"\"\"Gets user input for the prediction features.\"\"\"\n",
        "    print(\"Please enter the following input features:\")\n",
        "\n",
        "    days_testing = int(input(\"Enter days_testing (7, 28, or 90): \"))\n",
        "    fly_ash = float(input(\"Enter fly_ash: \"))\n",
        "    ggbs = float(input(\"Enter ggbs: \"))\n",
        "    sodium_silicate = float(input(\"Enter sodium_silicate: \"))\n",
        "    sodium_hydroxide = float(input(\"Enter sodium_hydroxide: \"))\n",
        "    sand = float(input(\"Enter sand: \"))\n",
        "    coarse_aggregate = float(input(\"Enter coarse_aggregate: \"))\n",
        "    glass_waste = float(input(\"Enter glass_waste: \"))\n",
        "    curing_type = input(\"Enter curing_type: \")\n",
        "\n",
        "    user_inputs = {\n",
        "        'days_testing': days_testing,\n",
        "        'fly_ash': fly_ash,\n",
        "        'ggbs': ggbs,\n",
        "        'sodium_silicate': sodium_silicate,\n",
        "        'sodium_hydroxide': sodium_hydroxide,\n",
        "        'sand': sand,\n",
        "        'coarse_aggregate': coarse_aggregate,\n",
        "        'glass_waste': glass_waste,\n",
        "        'curing_type': curing_type\n",
        "    }\n",
        "    return user_inputs\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0abc8c4b"
      },
      "source": [
        "## Prepare and scale data\n",
        "\n",
        "### Subtask:\n",
        "Create a function to prepare the input data into a DataFrame and scale it using the loaded scaler.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c00dcab"
      },
      "source": [
        "def prepare_and_scale_data(user_inputs, scaler):\n",
        "    \"\"\"\n",
        "    Prepares user input into a DataFrame and scales numerical features.\n",
        "\n",
        "    Args:\n",
        "        user_inputs: A dictionary containing user input features.\n",
        "        scaler: A pre-trained scaler object.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame with scaled numerical features.\n",
        "    \"\"\"\n",
        "    # Create a DataFrame from user inputs\n",
        "    df = pd.DataFrame([user_inputs])\n",
        "\n",
        "    # Rename columns to match required names\n",
        "    df = df.rename(columns={\n",
        "        'days_testing': 'Number of days (testing)',\n",
        "        'fly_ash': 'fly_ash_percentage',\n",
        "        'ggbs': 'ggbs_percentage',\n",
        "        'sodium_silicate': 'sodium_silicate_percentage',\n",
        "        'sodium_hydroxide': 'sodium_hydroxide_percentage',\n",
        "        'sand': 'sand_percentage',\n",
        "        'coarse_aggregate': 'coarse_aggregate_percentage',\n",
        "        'glass_waste': 'glass_waste_percentage',\n",
        "        'curing_type': 'Curing_type'\n",
        "    })\n",
        "\n",
        "\n",
        "    # Scale the numerical features\n",
        "    df = scaler.transform(df)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "977e4c79"
      },
      "source": [
        "## Make predictions\n",
        "\n",
        "### Subtask:\n",
        "Create a function to make predictions using each of the loaded models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ede59967"
      },
      "source": [
        "def make_predictions(data, models):\n",
        "    \"\"\"\n",
        "    Makes predictions using a dictionary of loaded models.\n",
        "\n",
        "    Args:\n",
        "        data: A pandas DataFrame with scaled input features.\n",
        "        models: A dictionary of pre-trained models.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing predictions from each model.\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "    for model_name, model in models.items():\n",
        "        try:\n",
        "            # The gpr model expects a 2D array, so we need to reshape the input\n",
        "            if \"gpr\" in model_name:\n",
        "                prediction = model.predict(data.values.reshape(-1, 1))\n",
        "            else:\n",
        "                prediction = model.predict(data)\n",
        "            predictions[model_name] = prediction[0]\n",
        "        except Exception:\n",
        "            print(f\"Model '{model_name}' is not fitted. Skipping prediction.\")\n",
        "    return predictions"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1569733f"
      },
      "source": [
        "## Display results\n",
        "\n",
        "### Subtask:\n",
        "Display the predictions from all the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e1e5161"
      },
      "source": [
        "def display_predictions(predictions):\n",
        "    \"\"\"\n",
        "    Displays the predictions from different models.\n",
        "\n",
        "    Args:\n",
        "        predictions: A dictionary containing predictions from each model.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Predictions ---\")\n",
        "    for model_name, prediction in predictions.items():\n",
        "        print(f\"{model_name}: {prediction:.2f}\")\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8959aac",
        "outputId": "3eed6792-c45f-4576-a29a-ed82eb594e9a"
      },
      "source": [
        "# Get user input\n",
        "user_input = get_user_input()\n",
        "\n",
        "# Prepare and scale the data\n",
        "scaled_data = prepare_and_scale_data(user_input, scaler)\n",
        "\n",
        "# Make predictions\n",
        "predictions = make_predictions(scaled_data, loaded_models)\n",
        "\n",
        "# Display predictions\n",
        "display_predictions(predictions)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the following input features:\n",
            "Enter days_testing (7, 28, or 90): 28\n",
            "Enter fly_ash: 10\n",
            "Enter ggbs: 8.5\n",
            "Enter sodium_silicate: 2.25\n",
            "Enter sodium_hydroxide: 2.25\n",
            "Enter sand: 25\n",
            "Enter coarse_aggregate: 50\n",
            "Enter glass_waste: 2\n",
            "Enter curing_type: 1\n",
            "Model 'ada_boost_model.pkl' is not fitted. Skipping prediction.\n",
            "Model 'decision_tree_model.pkl' is not fitted. Skipping prediction.\n",
            "Model 'gbr_model.pkl' is not fitted. Skipping prediction.\n",
            "Model 'gpr_model.pkl' is not fitted. Skipping prediction.\n",
            "Model 'gpr_grid_search_model.pkl' is not fitted. Skipping prediction.\n",
            "Model 'random_forest_model.pkl' is not fitted. Skipping prediction.\n",
            "Model 'scaler.pkl' is not fitted. Skipping prediction.\n",
            "\n",
            "--- Predictions ---\n",
            "ada_boost_grid_search_model.pkl: 30.21\n",
            "decision_tree_grid_search_model.pkl: 30.21\n",
            "gbr_grid_search_model.pkl: 38.13\n",
            "knn_model.pkl: 33.52\n",
            "knn_grid_search_model.pkl: 26.79\n",
            "mlp_model.pkl: 125.76\n",
            "mlp_grid_search_model.pkl: 126.66\n",
            "random_forest_grid_search_model.pkl: 29.53\n",
            "ridge_model.pkl: 6.85\n",
            "svr_model.pkl: 24.15\n",
            "svr_grid_search_model.pkl: 27.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "22"
      ],
      "metadata": {
        "id": "Vu2jEa6xaRmW"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}